# üöÄ H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI CHI TI·∫æT: H·ªÜ TH·ªêNG PH√ÇN T√çCH LIVESTREAM REALTIME

## üìã M·ª§C L·ª§C
1. [Ki·∫øn tr√∫c H·ªá th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
2. [Giai ƒëo·∫°n 1: Infrastructure & Ingestion](#giai-ƒëo·∫°n-1-infrastructure--ingestion)
3. [Giai ƒëo·∫°n 2: Stream Processing & Cleaning](#giai-ƒëo·∫°n-2-stream-processing--cleaning)
4. [Giai ƒëo·∫°n 3: AI Integration & Window Operations](#giai-ƒëo·∫°n-3-ai-integration--window-operations)
5. [Giai ƒëo·∫°n 4: Fault Tolerance & Storage](#giai-ƒëo·∫°n-4-fault-tolerance--storage)
6. [Dashboard Visualization](#dashboard-visualization)
7. [Testing & Validation](#testing--validation)

---

## üèóÔ∏è KI·∫æN TR√öC H·ªÜ TH·ªêNG

### T·ªïng quan 4 t·∫ßng:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  T·∫ßng 1: Data Source                                       ‚îÇ
‚îÇ  ‚îî‚îÄ Kafka (Buffer cho comment stream)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  T·∫ßng 2: Processing                                        ‚îÇ
‚îÇ  ‚îî‚îÄ Spark Streaming (DStream, micro-batches 1-2s)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  T·∫ßng 3: Analytics (AI)                                    ‚îÇ
‚îÇ  ‚îî‚îÄ Spark MLlib (Logistic Regression / Naive Bayes)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  T·∫ßng 4: Storage/Serving                                   ‚îÇ
‚îÇ  ‚îî‚îÄ HDFS (Checkpointing) + MongoDB/MySQL (Results)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### C√¥ng ngh·ªá s·ª≠ d·ª•ng:
- **Hadoop Ecosystem**: HDFS, YARN
- **Spark**: Spark Streaming, Spark MLlib, Spark SQL
- **Kafka**: Message Queue
- **Docker**: Containerization
- **Python**: Producer script, Dashboard
- **MongoDB/MySQL**: Storage k·∫øt qu·∫£

---

## üì¶ GIAI ƒêO·∫†N 1: INFRASTRUCTURE & INGESTION

### M·ª•c ti√™u:
- D·ª±ng c·ª•m Hadoop/Spark cluster
- C√†i ƒë·∫∑t Kafka
- Vi·∫øt Producer script gi·∫£ l·∫≠p comment livestream

### B∆∞·ªõc 1.1: Thi·∫øt l·∫≠p Docker Compose

T·∫°o file `docker-compose.yml`:

```yaml
version: '3.8'

services:
  # Zookeeper (Required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # HDFS NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "9000:9000"

  # HDFS DataNode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    environment:
      SERVICE_PRECONDITION: namenode:9870
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    ports:
      - "9864:9864"

  # Spark Master
  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    environment:
      - INIT_DAEMON_STEP=setup_spark
    ports:
      - "8080:8080"
      - "7077:7077"

  # Spark Worker
  spark-worker:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"

volumes:
  hadoop_namenode:
  hadoop_datanode:
```

### B∆∞·ªõc 1.2: Kh·ªüi ƒë·ªông Docker Compose

```bash
# Di chuy·ªÉn v√†o th∆∞ m·ª•c ch·ª©a docker-compose.yml
cd D:\Big_data

# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services
docker-compose up -d

# Ki·ªÉm tra tr·∫°ng th√°i
docker-compose ps

# Xem logs n·∫øu c√≥ l·ªói
docker-compose logs kafka
docker-compose logs spark-master
```

### B∆∞·ªõc 1.3: Ki·ªÉm tra c√°c services

```bash
# Ki·ªÉm tra Kafka
docker exec -it big_data_kafka_1 kafka-topics --list --bootstrap-server localhost:9092

# Ki·ªÉm tra HDFS NameNode
# M·ªü browser: http://localhost:9870

# Ki·ªÉm tra Spark Master
# M·ªü browser: http://localhost:8080
```

### B∆∞·ªõc 1.4: T·∫°o Kafka Topic

```bash
# T·∫°o topic cho livestream comments
docker exec -it big_data_kafka_1 kafka-topics --create \
  --topic livestream-comments \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# Ki·ªÉm tra topic ƒë√£ t·∫°o
docker exec -it big_data_kafka_1 kafka-topics --list --bootstrap-server localhost:9092
```

### B∆∞·ªõc 1.5: Vi·∫øt Producer Script (Python)

T·∫°o file `producer.py`:

```python
import json
import time
import random
from kafka import KafkaProducer
from datetime import datetime

# Danh s√°ch comment m·∫´u (gi·∫£ l·∫≠p livestream)
COMMENTS_POOL = [
    "Ch·ªët ƒë∆°n", "Gi√° bao nhi√™u", "Ship kh√¥ng", "C√≥ freeship kh√¥ng",
    "S·∫£n ph·∫©m ƒë·∫πp qu√°", "M√†u g√¨ c√≥ s·∫µn", "Size n√†o c√≥", "Ch·∫•t l∆∞·ª£ng t·ªët kh√¥ng",
    "Lag qu√°", "Kh√¥ng nghe ti·∫øng", "M√†n h√¨nh m·ªù", "ƒê·∫∑t h√†ng nh∆∞ th·∫ø n√†o",
    "Tuy·ªát v·ªùi", "R·∫•t th√≠ch", "Mua ngay", "Review chi ti·∫øt ƒëi",
    "L·ª´a ƒë·∫£o", "H√†ng gi·∫£", "Kh√¥ng ƒë√∫ng m√¥ t·∫£", "Tr·∫£ h√†ng ƒë∆∞·ª£c kh√¥ng"
]

# Topics hot
TOPICS = ["iPhone16", "VinFast", "ChatGPT", "Samsung", "Xiaomi"]

def generate_comment():
    """T·∫°o comment ng·∫´u nhi√™n"""
    comment = random.choice(COMMENTS_POOL)
    topic = random.choice(TOPICS)
    timestamp = datetime.now().isoformat()
    
    return {
        "comment_id": f"cmt_{int(time.time() * 1000)}",
        "content": comment,
        "topic": topic,
        "timestamp": timestamp,
        "user_id": f"user_{random.randint(1, 1000)}"
    }

def main():
    # K·∫øt n·ªëi Kafka Producer
    producer = KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    print("üöÄ B·∫Øt ƒë·∫ßu g·ª≠i comment v√†o Kafka...")
    print("Topic: livestream-comments")
    print("Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng\n")
    
    try:
        while True:
            # T·∫°o comment ng·∫´u nhi√™n
            comment_data = generate_comment()
            
            # G·ª≠i v√†o Kafka
            producer.send('livestream-comments', value=comment_data)
            
            print(f"[{comment_data['timestamp']}] Topic: {comment_data['topic']} | Comment: {comment_data['content']}")
            
            # Gi·∫£ l·∫≠p t·ªëc ƒë·ªô comment (1-3 gi√¢y/comment)
            time.sleep(random.uniform(1, 3))
            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  D·ª´ng Producer...")
        producer.close()

if __name__ == "__main__":
    main()
```

### B∆∞·ªõc 1.6: C√†i ƒë·∫∑t dependencies cho Producer

T·∫°o file `requirements.txt`:

```txt
kafka-python==2.0.2
```

C√†i ƒë·∫∑t:

```bash
pip install -r requirements.txt
```

### B∆∞·ªõc 1.7: Ch·∫°y Producer

```bash
python producer.py
```

### ‚úÖ Deliverable Giai ƒëo·∫°n 1:
- ‚úÖ Docker cluster ho·∫°t ƒë·ªông: Kafka, Spark, HDFS
- ‚úÖ Kafka topic `livestream-comments` ƒë√£ t·∫°o
- ‚úÖ Producer script g·ª≠i comment li√™n t·ª•c v√†o Kafka
- ‚úÖ C√≥ th·ªÉ ki·ªÉm tra b·∫±ng Kafka Consumer:

```bash
docker exec -it big_data_kafka_1 kafka-console-consumer \
  --topic livestream-comments \
  --from-beginning \
  --bootstrap-server localhost:9092
```

---

## üîÑ GIAI ƒêO·∫†N 2: STREAM PROCESSING & CLEANING

### M·ª•c ti√™u:
- Kh·ªüi t·∫°o Spark Streaming
- K·∫øt n·ªëi v·ªõi Kafka
- Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Preprocessing)

### B∆∞·ªõc 2.1: C√†i ƒë·∫∑t PySpark v√† dependencies

T·∫°o file `requirements-spark.txt`:

```txt
pyspark==3.3.0
kafka-python==2.0.2
findspark==2.0.1
```

C√†i ƒë·∫∑t:

```bash
pip install -r requirements-spark.txt
```

**L∆∞u √Ω**: C·∫ßn download Spark Kafka connector JAR:
- T·∫£i file: `spark-sql-kafka-0-10_2.12-3.3.0.jar`
- ƒê·∫∑t v√†o th∆∞ m·ª•c `jars/` ho·∫∑c ch·ªâ ƒë·ªãnh khi ch·∫°y Spark

### B∆∞·ªõc 2.2: Vi·∫øt Spark Streaming Code

T·∫°o file `spark_streaming.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

# Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("LivestreamCommentAnalysis") \
    .master("spark://localhost:7077") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .getOrCreate()

# T·∫Øt logging kh√¥ng c·∫ßn thi·∫øt
spark.sparkContext.setLogLevel("WARN")

# ƒê·ªãnh nghƒ©a schema cho comment
comment_schema = StructType([
    StructField("comment_id", StringType(), True),
    StructField("content", StringType(), True),
    StructField("topic", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("user_id", StringType(), True)
])

# ƒê·ªçc stream t·ª´ Kafka
kafka_stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "livestream-comments") \
    .option("startingOffsets", "latest") \
    .load()

# Parse JSON t·ª´ Kafka value
parsed_stream = kafka_stream.select(
    from_json(col("value").cast("string"), comment_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")

# Ti·ªÅn x·ª≠ l√Ω (Preprocessing)
def preprocess_comments(df, epoch_id):
    """
    Transformations:
    - flatMap: T√°ch c√¢u th√†nh t·ª´
    - filter: Lo·∫°i b·ªè comment r√°c/qu√° ng·∫Øn
    - map: Chu·∫©n h√≥a d·ªØ li·ªáu
    """
    # Lo·∫°i b·ªè comment null ho·∫∑c r·ªóng
    cleaned_df = df.filter(
        (col("content").isNotNull()) & 
        (length(col("content")) > 2)
    )
    
    # T√°ch t·ª´ (Word Tokenization)
    words_df = cleaned_df.select(
        col("comment_id"),
        col("content"),
        col("topic"),
        col("timestamp"),
        explode(split(col("content"), " ")).alias("word")
    )
    
    # Lo·∫°i b·ªè t·ª´ d·ª´ng (stop words) v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    stop_words = ["kh√¥ng", "c√≥", "g√¨", "n√†o", "ƒë∆∞·ª£c", "v√†", "c·ªßa", "cho"]
    filtered_words = words_df.filter(
        ~col("word").isin(stop_words) &
        (length(col("word")) > 1) &
        (col("word").rlike("^[a-zA-Z√Ä-·ªπ0-9]+$"))
    )
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print(f"\n=== Batch {epoch_id} ===")
    print(f"S·ªë l∆∞·ª£ng comment: {cleaned_df.count()}")
    print(f"S·ªë l∆∞·ª£ng t·ª´ sau khi l√†m s·∫°ch: {filtered_words.count()}")
    
    # L∆∞u v√†o console ƒë·ªÉ ki·ªÉm tra
    cleaned_df.select("comment_id", "content", "topic", "timestamp") \
        .show(10, truncate=False)

# √Åp d·ª•ng preprocessing
query = parsed_stream.writeStream \
    .foreachBatch(preprocess_comments) \
    .outputMode("update") \
    .start()

# Ch·ªù stream ch·∫°y
query.awaitTermination()
```

### B∆∞·ªõc 2.3: Ch·∫°y Spark Streaming

```bash
# ƒê·∫£m b·∫£o Producer ƒëang ch·∫°y ·ªü terminal kh√°c
# Terminal 1: python producer.py

# Terminal 2: Ch·∫°y Spark Streaming
python spark_streaming.py
```

### B∆∞·ªõc 2.4: Ki·ªÉm tra k·∫øt qu·∫£

- Xem logs trong console ƒë·ªÉ th·∫•y s·ªë l∆∞·ª£ng comment v√† t·ª´ ƒë√£ x·ª≠ l√Ω
- Ki·ªÉm tra Spark UI: http://localhost:8080

### ‚úÖ Deliverable Giai ƒëo·∫°n 2:
- ‚úÖ Spark Streaming ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ Kafka
- ‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch (lo·∫°i b·ªè spam, stop words)
- ‚úÖ Pipeline x·ª≠ l√Ω ch·∫°y ·ªïn ƒë·ªãnh v·ªõi micro-batches

---

## ü§ñ GIAI ƒêO·∫†N 3: AI INTEGRATION & WINDOW OPERATIONS

### M·ª•c ti√™u:
- Train v√† so s√°nh 2 m√¥ h√¨nh MLlib (Logistic Regression vs Naive Bayes)
- √Åp d·ª•ng sentiment analysis v√†o stream
- Window Operations ƒë·ªÉ t√≠nh xu h∆∞·ªõng

### B∆∞·ªõc 3.1: Chu·∫©n b·ªã Dataset cho Training

T·∫°o file `prepare_training_data.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF
from pyspark.ml import Pipeline
import json

spark = SparkSession.builder \
    .appName("PrepareTrainingData") \
    .master("local[*]") \
    .getOrCreate()

# T·∫°o dataset m·∫´u v·ªõi label (1: positive, 0: negative)
training_data = [
    ("Ch·ªët ƒë∆°n", 1),
    ("S·∫£n ph·∫©m ƒë·∫πp qu√°", 1),
    ("Tuy·ªát v·ªùi", 1),
    ("R·∫•t th√≠ch", 1),
    ("Mua ngay", 1),
    ("Ch·∫•t l∆∞·ª£ng t·ªët", 1),
    ("Lag qu√°", 0),
    ("Kh√¥ng nghe ti·∫øng", 0),
    ("L·ª´a ƒë·∫£o", 0),
    ("H√†ng gi·∫£", 0),
    ("Kh√¥ng ƒë√∫ng m√¥ t·∫£", 0),
    ("Tr·∫£ h√†ng", 0),
    ("Gi√° bao nhi√™u", 1),
    ("Ship kh√¥ng", 1),
    ("C√≥ freeship kh√¥ng", 1),
    ("M√†n h√¨nh m·ªù", 0),
    ("ƒê·∫∑t h√†ng nh∆∞ th·∫ø n√†o", 1),
    ("Review chi ti·∫øt", 1)
]

# T·∫°o DataFrame
df = spark.createDataFrame(training_data, ["comment", "label"])

# Feature Engineering Pipeline
tokenizer = Tokenizer(inputCol="comment", outputCol="words")
stop_words_remover = StopWordsRemover(
    inputCol="words", 
    outputCol="filtered_words",
    stopWords=["kh√¥ng", "c√≥", "g√¨", "n√†o", "ƒë∆∞·ª£c", "v√†", "c·ªßa", "cho"]
)
count_vectorizer = CountVectorizer(
    inputCol="filtered_words", 
    outputCol="raw_features",
    vocabSize=1000
)
idf = IDF(inputCol="raw_features", outputCol="features")

pipeline = Pipeline(stages=[tokenizer, stop_words_remover, count_vectorizer, idf])
model = pipeline.fit(df)
processed_df = model.transform(df)

# L∆∞u pipeline v√† data
model.write().overwrite().save("models/preprocessing_pipeline")
processed_df.write.mode("overwrite").parquet("data/training_data.parquet")

print("‚úÖ ƒê√£ chu·∫©n b·ªã training data!")
processed_df.select("comment", "label", "features").show(5, truncate=False)
```

### B∆∞·ªõc 3.2: Train v√† So s√°nh 2 Models

T·∫°o file `train_and_compare_models.py`:

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression, NaiveBayes
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import PipelineModel

spark = SparkSession.builder \
    .appName("ModelComparison") \
    .master("local[*]") \
    .getOrCreate()

# Load preprocessed data
processed_df = spark.read.parquet("data/training_data.parquet")

# Split train/test
train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)

print("=" * 60)
print("TRAINING MODELS")
print("=" * 60)

# Model 1: Logistic Regression
print("\n1. Training Logistic Regression...")
lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10)
lr_model = lr.fit(train_df)
lr_predictions = lr_model.transform(test_df)

# Model 2: Naive Bayes
print("2. Training Naive Bayes...")
nb = NaiveBayes(featuresCol="features", labelCol="label")
nb_model = nb.fit(train_df)
nb_predictions = nb_model.transform(test_df)

# ƒê√°nh gi√° Models
evaluator_acc = MulticlassClassificationEvaluator(
    labelCol="label", 
    predictionCol="prediction", 
    metricName="accuracy"
)

evaluator_auc = BinaryClassificationEvaluator(
    labelCol="label",
    rawPredictionCol="rawPrediction",
    metricName="areaUnderROC"
)

# K·∫øt qu·∫£ Logistic Regression
lr_accuracy = evaluator_acc.evaluate(lr_predictions)
lr_auc = evaluator_auc.evaluate(lr_predictions)

# K·∫øt qu·∫£ Naive Bayes
nb_accuracy = evaluator_acc.evaluate(nb_predictions)
nb_auc = evaluator_auc.evaluate(nb_predictions)

print("\n" + "=" * 60)
print("MODEL COMPARISON RESULTS")
print("=" * 60)
print(f"\nLogistic Regression:")
print(f"  - Accuracy: {lr_accuracy:.4f}")
print(f"  - AUC-ROC: {lr_auc:.4f}")

print(f"\nNaive Bayes:")
print(f"  - Accuracy: {nb_accuracy:.4f}")
print(f"  - AUC-ROC: {nb_auc:.4f}")

# Ch·ªçn model t·ªët h∆°n
if lr_accuracy >= nb_accuracy:
    best_model = lr_model
    best_model_name = "LogisticRegression"
    print(f"\n‚úÖ Model t·ªët nh·∫•t: Logistic Regression (Accuracy: {lr_accuracy:.4f})")
else:
    best_model = nb_model
    best_model_name = "NaiveBayes"
    print(f"\n‚úÖ Model t·ªët nh·∫•t: Naive Bayes (Accuracy: {nb_accuracy:.4f})")

# L∆∞u model t·ªët nh·∫•t
best_model.write().overwrite().save(f"models/{best_model_name}_model")
print(f"\nüíæ ƒê√£ l∆∞u model: models/{best_model_name}_model")

# Test predictions
print("\n" + "=" * 60)
print("SAMPLE PREDICTIONS")
print("=" * 60)
test_df.select("comment", "label").show(5)
lr_predictions.select("comment", "label", "prediction", "probability").show(5, truncate=False)
```

### B∆∞·ªõc 3.3: T√≠ch h·ª£p Model v√†o Spark Streaming

T·∫°o file `spark_streaming_with_ml.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import PipelineModel
from pyspark.ml.classification import LogisticRegressionModel
import json

# Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("LivestreamSentimentAnalysis") \
    .master("spark://localhost:7077") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Load preprocessing pipeline v√† model
preprocessing_pipeline = PipelineModel.load("models/preprocessing_pipeline")
sentiment_model = LogisticRegressionModel.load("models/LogisticRegression_model")

# Schema cho comment
comment_schema = StructType([
    StructField("comment_id", StringType(), True),
    StructField("content", StringType(), True),
    StructField("topic", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("user_id", StringType(), True)
])

# ƒê·ªçc stream t·ª´ Kafka
kafka_stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "livestream-comments") \
    .option("startingOffsets", "latest") \
    .load()

# Parse JSON
parsed_stream = kafka_stream.select(
    from_json(col("value").cast("string"), comment_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")

def process_with_sentiment(df, epoch_id):
    """
    X·ª≠ l√Ω stream v·ªõi sentiment analysis v√† window operations
    """
    # L√†m s·∫°ch d·ªØ li·ªáu
    cleaned_df = df.filter(
        (col("content").isNotNull()) & 
        (length(col("content")) > 2)
    )
    
    if cleaned_df.count() == 0:
        return
    
    # √Åp d·ª•ng preprocessing pipeline
    preprocessed_df = preprocessing_pipeline.transform(cleaned_df)
    
    # D·ª± ƒëo√°n sentiment
    predictions_df = sentiment_model.transform(preprocessed_df)
    
    # Chuy·ªÉn ƒë·ªïi prediction (0/1) th√†nh sentiment label
    sentiment_df = predictions_df.withColumn(
        "sentiment",
        when(col("prediction") == 1.0, "positive")
        .otherwise("negative")
    ).withColumn(
        "sentiment_score",
        col("probability")[1] - col("probability")[0]  # positive - negative
    )
    
    # Window Operations: T√≠nh sentiment trong 30 gi√¢y g·∫ßn nh·∫•t
    windowed_df = sentiment_df \
        .withWatermark("kafka_timestamp", "30 seconds") \
        .groupBy(
            window(col("kafka_timestamp"), "30 seconds", "10 seconds"),
            col("topic")
        ) \
        .agg(
            count("*").alias("comment_count"),
            avg("sentiment_score").alias("avg_sentiment"),
            sum(when(col("sentiment") == "positive", 1).otherwise(0)).alias("positive_count"),
            sum(when(col("sentiment") == "negative", 1).otherwise(0)).alias("negative_count")
        )
    
    # Top Comments: ƒê·∫øm t·∫ßn su·∫•t comment
    top_comments_df = sentiment_df \
        .withWatermark("kafka_timestamp", "1 minute") \
        .groupBy(
            window(col("kafka_timestamp"), "1 minute", "10 seconds"),
            col("content")
        ) \
        .agg(count("*").alias("frequency")) \
        .orderBy(desc("frequency")) \
        .limit(5)
    
    # Top Negative Comments
    top_negative_df = sentiment_df \
        .filter(col("sentiment") == "negative") \
        .withWatermark("kafka_timestamp", "1 minute") \
        .groupBy(
            window(col("kafka_timestamp"), "1 minute", "10 seconds"),
            col("content")
        ) \
        .agg(count("*").alias("frequency")) \
        .orderBy(desc("frequency")) \
        .limit(5)
    
    # Keywords: Word Count
    words_df = sentiment_df.select(
        explode(split(col("content"), " ")).alias("word"),
        col("kafka_timestamp")
    ).filter(
        (length(col("word")) > 2) &
        (col("word").rlike("^[a-zA-Z√Ä-·ªπ0-9]+$"))
    )
    
    top_keywords_df = words_df \
        .withWatermark("kafka_timestamp", "1 minute") \
        .groupBy(
            window(col("kafka_timestamp"), "1 minute", "10 seconds"),
            col("word")
        ) \
        .agg(count("*").alias("frequency")) \
        .orderBy(desc("frequency")) \
        .limit(10)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print(f"\n{'='*60}")
    print(f"BATCH {epoch_id} - SENTIMENT ANALYSIS RESULTS")
    print(f"{'='*60}")
    
    print("\nüìä Sentiment trong 30 gi√¢y g·∫ßn nh·∫•t:")
    windowed_df.select("window", "topic", "comment_count", "avg_sentiment", 
                      "positive_count", "negative_count").show(truncate=False)
    
    print("\nüî• Top Comments:")
    top_comments_df.select("window", "content", "frequency").show(truncate=False)
    
    print("\n‚ö†Ô∏è  Top Negative Comments:")
    top_negative_df.select("window", "content", "frequency").show(truncate=False)
    
    print("\nüîë Top Keywords:")
    top_keywords_df.select("window", "word", "frequency").show(truncate=False)
    
    # L∆∞u v√†o memory table ƒë·ªÉ Dashboard ƒë·ªçc
    sentiment_df.select("comment_id", "content", "sentiment", "sentiment_score", 
                       "topic", "kafka_timestamp") \
        .write \
        .format("memory") \
        .option("table", "sentiment_results") \
        .mode("append") \
        .save()

# Ch·∫°y stream processing
query = parsed_stream.writeStream \
    .foreachBatch(process_with_sentiment) \
    .outputMode("update") \
    .trigger(processingTime='10 seconds') \
    .start()

query.awaitTermination()
```

### B∆∞·ªõc 3.4: Ch·∫°y c√°c scripts

```bash
# Terminal 1: Chu·∫©n b·ªã training data
python prepare_training_data.py

# Terminal 2: Train v√† so s√°nh models
python train_and_compare_models.py

# Terminal 3: Ch·∫°y Producer
python producer.py

# Terminal 4: Ch·∫°y Spark Streaming v·ªõi ML
python spark_streaming_with_ml.py
```

### ‚úÖ Deliverable Giai ƒëo·∫°n 3:
- ‚úÖ B√°o c√°o so s√°nh Logistic Regression vs Naive Bayes
- ‚úÖ Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u v√† s·ª≠ d·ª•ng
- ‚úÖ Sentiment analysis realtime tr√™n DStream
- ‚úÖ Window Operations t√≠nh xu h∆∞·ªõng 30 gi√¢y
- ‚úÖ Top Comments, Top Negative, Top Keywords ƒë∆∞·ª£c t√≠nh to√°n

---

## üíæ GIAI ƒêO·∫†N 4: FAULT TOLERANCE & STORAGE

### M·ª•c ti√™u:
- C·∫•u h√¨nh Checkpointing ƒë·ªÉ ph·ª•c h·ªìi khi l·ªói
- L∆∞u tr·ªØ k·∫øt qu·∫£ v√†o Database
- ƒê·∫£m b·∫£o h·ªá th·ªëng resilient

### B∆∞·ªõc 4.1: C·∫•u h√¨nh Checkpointing

T·∫°o file `spark_streaming_checkpoint.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import PipelineModel
from pyspark.ml.classification import LogisticRegressionModel

# Kh·ªüi t·∫°o Spark Session v·ªõi checkpoint directory
spark = SparkSession.builder \
    .appName("LivestreamSentimentAnalysis") \
    .master("spark://localhost:7077") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .config("spark.sql.streaming.checkpointLocation", "hdfs://namenode:9000/checkpoints/livestream") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Checkpoint directory tr√™n HDFS
checkpoint_location = "hdfs://namenode:9000/checkpoints/livestream"

# Load models
preprocessing_pipeline = PipelineModel.load("models/preprocessing_pipeline")
sentiment_model = LogisticRegressionModel.load("models/LogisticRegression_model")

# Schema
comment_schema = StructType([
    StructField("comment_id", StringType(), True),
    StructField("content", StringType(), True),
    StructField("topic", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("user_id", StringType(), True)
])

# ƒê·ªçc stream t·ª´ Kafka
kafka_stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "livestream-comments") \
    .option("startingOffsets", "latest") \
    .load()

parsed_stream = kafka_stream.select(
    from_json(col("value").cast("string"), comment_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")

def process_and_save(df, epoch_id):
    """X·ª≠ l√Ω v√† l∆∞u k·∫øt qu·∫£"""
    # ... (code x·ª≠ l√Ω gi·ªëng nh∆∞ b∆∞·ªõc 3.3)
    
    # L∆∞u v√†o HDFS (Parquet format)
    sentiment_df.write \
        .mode("append") \
        .parquet("hdfs://namenode:9000/results/sentiment")

# Stream v·ªõi checkpointing
query = parsed_stream.writeStream \
    .foreachBatch(process_and_save) \
    .outputMode("update") \
    .option("checkpointLocation", checkpoint_location) \
    .trigger(processingTime='10 seconds') \
    .start()

query.awaitTermination()
```

### B∆∞·ªõc 4.2: L∆∞u tr·ªØ v√†o MongoDB

C√†i ƒë·∫∑t MongoDB connector:

```bash
pip install pymongo
```

T·∫°o file `save_to_mongodb.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pymongo import MongoClient
import json

# K·∫øt n·ªëi MongoDB
mongo_client = MongoClient('mongodb://localhost:27017/')
db = mongo_client['livestream_analysis']
collection = db['sentiment_results']

def save_to_mongodb(df, epoch_id):
    """L∆∞u k·∫øt qu·∫£ v√†o MongoDB"""
    # Convert Spark DataFrame to Pandas
    pandas_df = df.toPandas()
    
    # Insert v√†o MongoDB
    records = pandas_df.to_dict('records')
    if records:
        collection.insert_many(records)
        print(f"‚úÖ ƒê√£ l∆∞u {len(records)} records v√†o MongoDB")

# S·ª≠ d·ª•ng trong foreachBatch
query = parsed_stream.writeStream \
    .foreachBatch(save_to_mongodb) \
    .outputMode("update") \
    .start()
```

### B∆∞·ªõc 4.3: L∆∞u tr·ªØ v√†o MySQL

T·∫°o file `save_to_mysql.py`:

```python
from pyspark.sql import SparkSession

# L∆∞u v√†o MySQL
def save_to_mysql(df, epoch_id):
    """L∆∞u k·∫øt qu·∫£ v√†o MySQL"""
    df.write \
        .format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/livestream_db") \
        .option("dbtable", "sentiment_results") \
        .option("user", "root") \
        .option("password", "password") \
        .mode("append") \
        .save()
    
    print(f"‚úÖ ƒê√£ l∆∞u batch {epoch_id} v√†o MySQL")
```

### ‚úÖ Deliverable Giai ƒëo·∫°n 4:
- ‚úÖ Checkpointing ƒë∆∞·ª£c c·∫•u h√¨nh tr√™n HDFS
- ‚úÖ H·ªá th·ªëng c√≥ th·ªÉ t·ª± ph·ª•c h·ªìi khi Driver crash
- ‚úÖ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u v√†o Database (MongoDB/MySQL)
- ‚úÖ D·ªØ li·ªáu l·ªãch s·ª≠ ƒë∆∞·ª£c l∆∞u tr√™n HDFS

---

## üìä DASHBOARD VISUALIZATION

### M·ª•c ti√™u:
- X√¢y d·ª±ng Dashboard realtime v·ªõi Streamlit
- Hi·ªÉn th·ªã Sentiment Gauge, Top Comments, Top Keywords

### B∆∞·ªõc 5.1: C√†i ƒë·∫∑t Streamlit

```bash
pip install streamlit pandas matplotlib seaborn plotly
```

### B∆∞·ªõc 5.2: T·∫°o Dashboard

T·∫°o file `dashboard.py`:

```python
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pymongo import MongoClient
import time
from datetime import datetime, timedelta

# K·∫øt n·ªëi MongoDB
mongo_client = MongoClient('mongodb://localhost:27017/')
db = mongo_client['livestream_analysis']
collection = db['sentiment_results']

st.set_page_config(page_title="Livestream Analytics Dashboard", layout="wide")

st.title("üìä Livestream Realtime Analytics Dashboard")

# Sidebar
st.sidebar.header("‚öôÔ∏è C√†i ƒë·∫∑t")
refresh_interval = st.sidebar.slider("Refresh Interval (seconds)", 5, 60, 10)
auto_refresh = st.sidebar.checkbox("Auto Refresh", value=True)

# Placeholder cho d·ªØ li·ªáu
placeholder = st.empty()

def get_latest_data():
    """L·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ MongoDB"""
    # L·∫•y d·ªØ li·ªáu trong 1 ph√∫t g·∫ßn nh·∫•t
    one_minute_ago = datetime.now() - timedelta(minutes=1)
    
    cursor = collection.find({
        "kafka_timestamp": {"$gte": one_minute_ago.isoformat()}
    })
    
    df = pd.DataFrame(list(cursor))
    return df

def calculate_metrics(df):
    """T√≠nh to√°n c√°c metrics"""
    if df.empty:
        return {
            "avg_sentiment": 0,
            "positive_count": 0,
            "negative_count": 0,
            "total_comments": 0,
            "top_comments": pd.DataFrame(),
            "top_keywords": pd.DataFrame()
        }
    
    avg_sentiment = df['sentiment_score'].mean() if 'sentiment_score' in df.columns else 0
    positive_count = len(df[df['sentiment'] == 'positive']) if 'sentiment' in df.columns else 0
    negative_count = len(df[df['sentiment'] == 'negative']) if 'sentiment' in df.columns else 0
    total_comments = len(df)
    
    # Top Comments
    top_comments = df.groupby('content').size().reset_index(name='frequency')
    top_comments = top_comments.sort_values('frequency', ascending=False).head(5)
    
    # Top Keywords (t√°ch t·ª´)
    if 'content' in df.columns:
        words = df['content'].str.split().explode()
        top_keywords = words.value_counts().head(10).reset_index()
        top_keywords.columns = ['keyword', 'frequency']
    else:
        top_keywords = pd.DataFrame()
    
    return {
        "avg_sentiment": avg_sentiment,
        "positive_count": positive_count,
        "negative_count": negative_count,
        "total_comments": total_comments,
        "top_comments": top_comments,
        "top_keywords": top_keywords
    }

def create_sentiment_gauge(score):
    """T·∫°o Sentiment Gauge"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = score,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Sentiment Score"},
        delta = {'reference': 0},
        gauge = {
            'axis': {'range': [-1, 1]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [-1, -0.5], 'color': "lightgray"},
                {'range': [-0.5, 0], 'color': "gray"},
                {'range': [0, 0.5], 'color': "lightgreen"},
                {'range': [0.5, 1], 'color': "green"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 0
            }
        }
    ))
    fig.update_layout(height=300)
    return fig

while True:
    with placeholder.container():
        # L·∫•y d·ªØ li·ªáu
        df = get_latest_data()
        metrics = calculate_metrics(df)
        
        # Row 1: Sentiment Gauge v√† Stats
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Comments", metrics["total_comments"])
        
        with col2:
            st.metric("Positive", metrics["positive_count"], delta=f"{metrics['positive_count']}")
        
        with col3:
            st.metric("Negative", metrics["negative_count"], delta=f"-{metrics['negative_count']}")
        
        with col4:
            st.metric("Avg Sentiment", f"{metrics['avg_sentiment']:.2f}")
        
        # Row 2: Sentiment Gauge
        st.subheader("üéØ Sentiment Gauge (Last 1 Minute)")
        gauge_fig = create_sentiment_gauge(metrics['avg_sentiment'])
        st.plotly_chart(gauge_fig, use_container_width=True)
        
        # Row 3: Top Comments v√† Top Keywords
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üî• Top Comments")
            if not metrics['top_comments'].empty:
                fig_comments = px.bar(
                    metrics['top_comments'],
                    x='frequency',
                    y='content',
                    orientation='h',
                    labels={'frequency': 'Frequency', 'content': 'Comment'}
                )
                st.plotly_chart(fig_comments, use_container_width=True)
            else:
                st.info("Ch∆∞a c√≥ d·ªØ li·ªáu")
        
        with col2:
            st.subheader("üîë Top Keywords")
            if not metrics['top_keywords'].empty:
                fig_keywords = px.bar(
                    metrics['top_keywords'],
                    x='frequency',
                    y='keyword',
                    orientation='h',
                    labels={'frequency': 'Frequency', 'keyword': 'Keyword'}
                )
                st.plotly_chart(fig_keywords, use_container_width=True)
            else:
                st.info("Ch∆∞a c√≥ d·ªØ li·ªáu")
        
        # Row 4: Top Negative Comments
        st.subheader("‚ö†Ô∏è Top Negative Comments")
        negative_df = df[df['sentiment'] == 'negative'] if 'sentiment' in df.columns else pd.DataFrame()
        if not negative_df.empty:
            negative_comments = negative_df.groupby('content').size().reset_index(name='frequency')
            negative_comments = negative_comments.sort_values('frequency', ascending=False).head(5)
            st.dataframe(negative_comments, use_container_width=True)
        else:
            st.info("Ch∆∞a c√≥ comment ti√™u c·ª±c")
        
        # Refresh
        if auto_refresh:
            time.sleep(refresh_interval)
            st.rerun()
        else:
            break
```

### B∆∞·ªõc 5.3: Ch·∫°y Dashboard

```bash
streamlit run dashboard.py
```

Dashboard s·∫Ω m·ªü t·∫°i: http://localhost:8501

### ‚úÖ Deliverable Dashboard:
- ‚úÖ Sentiment Gauge hi·ªÉn th·ªã c·∫£m x√∫c realtime
- ‚úÖ Top Comments bar chart
- ‚úÖ Top Keywords visualization
- ‚úÖ Top Negative Comments table
- ‚úÖ Auto-refresh m·ªói 10 gi√¢y

---

## üß™ TESTING & VALIDATION

### Ki·ªÉm tra t·ª´ng giai ƒëo·∫°n:

#### Test Giai ƒëo·∫°n 1:
```bash
# Ki·ªÉm tra Kafka topic
docker exec -it big_data_kafka_1 kafka-console-consumer \
  --topic livestream-comments \
  --from-beginning \
  --bootstrap-server localhost:9092

# Ki·ªÉm tra HDFS
docker exec -it namenode hdfs dfs -ls /
```

#### Test Giai ƒëo·∫°n 2:
- Xem Spark UI: http://localhost:8080
- Ki·ªÉm tra logs trong console

#### Test Giai ƒëo·∫°n 3:
- Ki·ªÉm tra model accuracy trong b√°o c√°o
- Xem predictions trong console output

#### Test Giai ƒëo·∫°n 4:
- Simulate crash: D·ª´ng Spark Streaming v√† restart
- Ki·ªÉm tra checkpoint recovery
- Ki·ªÉm tra data trong MongoDB/MySQL

### Troubleshooting:

**L·ªói Kafka connection:**
```bash
# Ki·ªÉm tra Kafka ƒëang ch·∫°y
docker ps | grep kafka

# Ki·ªÉm tra port
netstat -an | findstr 9092
```

**L·ªói Spark connection:**
```bash
# Ki·ªÉm tra Spark Master
curl http://localhost:8080

# Xem logs
docker logs spark-master
```

**L·ªói HDFS:**
```bash
# Ki·ªÉm tra NameNode
docker exec -it namenode hdfs dfsadmin -report
```

---

## üìù T·ªîNG K·∫æT

Sau khi ho√†n th√†nh t·∫•t c·∫£ c√°c giai ƒëo·∫°n, b·∫°n s·∫Ω c√≥:

‚úÖ **H·ªá th·ªëng Big Data ho√†n ch·ªânh:**
- Hadoop (HDFS/YARN) cluster
- Spark Streaming x·ª≠ l√Ω realtime
- Kafka message queue
- MLlib sentiment analysis
- Dashboard visualization

‚úÖ **K·∫øt qu·∫£ Dashboard:**
- Sentiment Gauge (ƒë·ªìng h·ªì c·∫£m x√∫c)
- Top Comments (comments ph·ªï bi·∫øn)
- Top Negative (c·∫£nh b√°o)
- Top Keywords (t·ª´ kh√≥a hot)

‚úÖ **Fault Tolerance:**
- Checkpointing tr√™n HDFS
- T·ª± ƒë·ªông ph·ª•c h·ªìi khi l·ªói

‚úÖ **B√°o c√°o Model Comparison:**
- So s√°nh Logistic Regression vs Naive Bayes
- Ch·ªçn model t·ªët nh·∫•t

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

- Spark Streaming Documentation: https://spark.apache.org/docs/latest/streaming-programming-guide.html
- Kafka Documentation: https://kafka.apache.org/documentation/
- MLlib Guide: https://spark.apache.org/docs/latest/ml-guide.html
- Streamlit Documentation: https://docs.streamlit.io/

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi d·ª± √°n! üöÄ**

